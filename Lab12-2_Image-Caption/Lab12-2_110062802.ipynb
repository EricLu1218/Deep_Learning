{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab12-2: Image Captioning\n",
    "<hr>\n",
    "\n",
    "110062802 呂宸漢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 22:53:01.867523: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), 'Physical GPUs,', len(logical_gpus), 'Logical GPUs')\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./words_captcha/a0.png\n",
      "<start> t h u s <end>\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "img_name_list = []\n",
    "cap_list = []\n",
    "\n",
    "with open('./words_captcha/spec_train_val.txt') as fin:\n",
    "    for line in fin:\n",
    "        image_name, caption = line.strip().split()\n",
    "        img_name_list.append(f'./words_captcha/{image_name}.png')\n",
    "        cap_list.append('<start> ' + ' '.join(caption) + ' <end>')\n",
    "\n",
    "test_img_name = set(glob.glob(f'./words_captcha/*.png')) - set(img_name_list)\n",
    "img_name_list += sorted(test_img_name)\n",
    "\n",
    "print(img_name_list[0])\n",
    "print(cap_list[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> t h u s <end>\n",
      "[ 2  9 18 17  6  3  0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                                  oov_token='')\n",
    "tokenizer.fit_on_texts(cap_list)\n",
    "cap_seqs = tokenizer.texts_to_sequences(cap_list)\n",
    "\n",
    "cap_seqs = tf.keras.preprocessing.sequence.pad_sequences(cap_seqs, padding='post')\n",
    "max_length = len(cap_seqs[0])\n",
    "\n",
    "print(cap_list[0])\n",
    "print(cap_seqs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data to Training, Validating, and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, img_name_valid = img_name_list[:100000], img_name_list[100000:120000]\n",
    "cap_seqs_train, cap_seqs_valid = cap_seqs[:100000], cap_seqs[100000:]\n",
    "\n",
    "img_name_test = img_name_list[120000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (160, 300)\n",
    "BATCH_SIZE = 50\n",
    "BUFFER_SIZE = 5000\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 512\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "EPOCHS = 10\n",
    "STEPS = len(img_name_train) // BATCH_SIZE\n",
    "LEARNING_RATE = 1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_train(img_path, cap_seq):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = img / 255 * 2 - 1\n",
    "    return img, cap_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((img_name_train, cap_seqs_train))\\\n",
    "                               .map(map_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .shuffle(BUFFER_SIZE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices((img_name_valid, cap_seqs_valid))\\\n",
    "                               .map(map_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class conv_leaky_relu(layers.Layer):\n",
    "    def __init__(self, filters, size, stride):\n",
    "        super(conv_leaky_relu, self).__init__()\n",
    "        self.conv_2d = layers.Conv2D(filters, size, stride, padding='same')\n",
    "        self.batch_norm = layers.BatchNormalization()\n",
    "        self.leakey_relu = layers.LeakyReLU(0.1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv_2d(inputs)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.leakey_relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, layers, Model\n",
    "\n",
    "inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "x = conv_leaky_relu(64, 7, 2)(inputs)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(192, 3, 1)(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(128, 1, 1)(x)\n",
    "x = conv_leaky_relu(256, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(512, 1, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(512, 1, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = conv_leaky_relu(512, 1, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 2)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "outputs = conv_leaky_relu(1024, 3, 1)(x)\n",
    "\n",
    "feature_extractor = Model(inputs=inputs, outputs=outputs, name='YOLO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"YOLO\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 160, 300, 3)]     0         \n",
      "                                                                 \n",
      " conv_leaky_relu (conv_leaky  (None, 80, 150, 64)      9728      \n",
      " _relu)                                                          \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 40, 75, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_leaky_relu_1 (conv_lea  (None, 40, 75, 192)      111552    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 20, 37, 192)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_2 (conv_lea  (None, 20, 37, 128)      25216     \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_3 (conv_lea  (None, 20, 37, 256)      296192    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_4 (conv_lea  (None, 20, 37, 256)      66816     \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_5 (conv_lea  (None, 20, 37, 512)      1182208   \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 10, 18, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_6 (conv_lea  (None, 10, 18, 256)      132352    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_7 (conv_lea  (None, 10, 18, 512)      1182208   \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_8 (conv_lea  (None, 10, 18, 256)      132352    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_9 (conv_lea  (None, 10, 18, 512)      1182208   \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_10 (conv_le  (None, 10, 18, 256)      132352    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_11 (conv_le  (None, 10, 18, 512)      1182208   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_12 (conv_le  (None, 10, 18, 256)      132352    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_13 (conv_le  (None, 10, 18, 512)      1182208   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_14 (conv_le  (None, 10, 18, 512)      264704    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_15 (conv_le  (None, 10, 18, 1024)     4723712   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 9, 1024)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_16 (conv_le  (None, 5, 9, 512)        526848    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_17 (conv_le  (None, 5, 9, 1024)       4723712   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_18 (conv_le  (None, 5, 9, 512)        526848    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_19 (conv_le  (None, 5, 9, 1024)       4723712   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_20 (conv_le  (None, 5, 9, 1024)       9442304   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_21 (conv_le  (None, 3, 5, 1024)       9442304   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_22 (conv_le  (None, 3, 5, 1024)       9442304   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_23 (conv_le  (None, 3, 5, 1024)       9442304   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,208,704\n",
      "Trainable params: 60,182,336\n",
      "Non-trainable params: 26,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape after passing through fc == (batch_size, 15, embedding_dim)\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 15, embedding_dim)\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 15, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 15, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector == (batch_size, embedding_dim)\n",
    "        context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # x shape == (batch_size, 1)\n",
    "        # features shape == (batch_size, 15, embedding_dim)\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "\n",
    "        # context_vector shape == (batch_size, embedding_dim)\n",
    "        # attention_weights shape == (batch_size, 15, 1)\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        # output shape == (batch_size, 1, hidden_size)\n",
    "        # state(hidden) shape == (batch_size, hidden_size)\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # x shape == (batch_size, 1, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(EMBEDDING_DIM)\n",
    "decoder = RNN_Decoder(EMBEDDING_DIM, UNITS, VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoints/YOLO/'\n",
    "ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = feature_extractor(img_tensor, True)\n",
    "        features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    mean_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = feature_extractor.trainable_variables + \\\n",
    "        encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_tensor):\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    dec_input = tf.expand_dims(\n",
    "        [tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "    features = feature_extractor(img_tensor)\n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    for _ in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(segs):\n",
    "    result_list = []\n",
    "    for seq in segs:\n",
    "        result = ''\n",
    "        for s in seq[1:]:\n",
    "            if s == tokenizer.word_index['<end>']:\n",
    "                break\n",
    "            result += tokenizer.index_word[s]\n",
    "        result_list.append(result)\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset_valid):\n",
    "    sample_count = 0\n",
    "    correct_count = 0\n",
    "    for img_tensor, target in dataset_valid:\n",
    "        pred_list = postprocess(predict(img_tensor).numpy())\n",
    "        real_list = postprocess(target.numpy())\n",
    "\n",
    "        for pred, real in zip(pred_list, real_list):\n",
    "            sample_count += 1\n",
    "            if pred == real:\n",
    "                correct_count += 1\n",
    "\n",
    "    return correct_count / sample_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 2000/2000 [03:43<00:00,  8.97it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 2000/2000 [03:28<00:00,  9.57it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 2000/2000 [03:28<00:00,  9.57it/s, loss=0.0457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 2000/2000 [03:29<00:00,  9.55it/s, loss=0.0298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 2000/2000 [03:28<00:00,  9.57it/s, loss=0.0208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 2000/2000 [04:21<00:00,  7.64it/s, loss=0.0177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 2000/2000 [03:32<00:00,  9.42it/s, loss=0.0154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 2000/2000 [03:32<00:00,  9.42it/s, loss=0.00996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 2000/2000 [03:32<00:00,  9.42it/s, loss=0.00993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2000/2000 [03:32<00:00,  9.42it/s, loss=0.00948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.98\n",
      "Time taken for 10 epoch 2517.595983028412 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "loss_plot = []\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    loss = 0\n",
    "    pbar = tqdm(dataset_train, total=STEPS, desc=f'Epoch {epoch + 1:2d}')\n",
    "    for (step, (img_tensor, target)) in enumerate(pbar):\n",
    "        loss += train_step(img_tensor, target)\n",
    "        pbar.set_postfix({'loss': loss.numpy() / (step + 1)})\n",
    "\n",
    "    loss_plot.append(loss / STEPS)\n",
    "    ckpt_manager.save()\n",
    "\n",
    "    score = evaluate(dataset_valid)\n",
    "    print(f'Validation accuracy: {score:.2f}')\n",
    "\n",
    "print('Time taken for {} epoch {} sec\\n'.format(EPOCHS - start_epoch, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f73842e4970>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt.restore('./checkpoints/YOLO/ckpt-10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "score = evaluate(dataset_valid)\n",
    "print(f'Validation accuracy: {score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_test(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = img / 255 * 2 - 1\n",
    "    return img, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((img_name_test))\\\n",
    "                              .map(map_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                              .batch(100)\\\n",
    "                              .prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:28<00:00,  6.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('./Lab12-2_110062802.txt', 'w') as fout:\n",
    "    for step, (img_tensor, img_path) in enumerate(tqdm(dataset_test)):\n",
    "        pred_list = postprocess(predict(img_tensor).numpy())\n",
    "        for path, pred in zip(img_path, pred_list):\n",
    "            path = path.numpy().decode('utf-8')\n",
    "            name = re.search('(a[0-9]+)', path).group(1)\n",
    "            fout.write(f'{name} {pred}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "我以YOLOv1 convolution的部分當作feature extractor，其餘encoder、attention與decoder的部分則與助教的相同。由於我的YOLOv1沒有pretrain的weight，因此在update weight的時候還需將YOLOv1的trainable_variables加進去更新。由training的log可以看出在第五個epoch後validation的準確率可以達90%以上，第十個epoch的準確率更可達到98%，因此我取第十個epoch的weight當作我最後的model weight，並用他來預測testing data。\n",
    "\n",
    "值得一提的是，在training的過程中並不是loss越低validation accuracy越高，可以看到在第六個epoch時明明loss也不錯可是accuracy只有1%，反而相鄰的兩個epoch accuracy都在90%以上，我認為這是因為本次題目是要求predict與target的字要完全相同，而loss無法準確反映在完全相同的字上面，導致loss與accuracy之間有落差，不過這也只是我的推測而已，也有機會是我model的設計有瑕疵導致，真正的原因仍需調查。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05369867b289fbcd469a5a13d459af9004edcaf479b6e68a71616af7c4a7fea3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
