{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 1: Predicting News Popularity\n",
    "Team Name: 沒liao恩宇  \n",
    "Team Member:  110062802 呂宸漢 110062552 周伯宇 110062560 林子鵑  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "讀入 Training data & Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('./dataset/train.csv')\n",
    "df_test = pd.read_csv('./dataset/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Data Cleaning\n",
    "BeautifulSoup 為 Python 的函式庫，可以從 HTML、XML 檔案中分析資料，從 Raw Data (News Content) 提取不同的 Tag 開頭來分析本次 Competition 中所使用到的 data。  \n",
    "以下分別介紹每一種 Data 的 Extraction and Cleaning 方式:  \n",
    "- **title**: 文章標題 \n",
    "- **author**: 文章作者 (去除 by, & 等多餘的雜訊，保留作者名稱的 lower case) \n",
    "- **channel**: 文章所屬的頻道 (article 中 data-channel)\n",
    "- **topic**: 文章所屬的章節主題 (footer 中 article-topics)\n",
    "- **date_time**: 文章所發表的日期時間 (包含年月日時分秒)，若文章中沒有此項資訊 default 設為 'Wed, 10 Oct 2014 15:00:43'\n",
    "- **see_also**: 文章中標記 see also 的數量\n",
    "- **content_Len**: 文章長度\n",
    "- **num_image**: 文章中圖片數量 (find_all 找出所有 HTML 標籤 'img' 出現次數)\n",
    "- **num_a**: 文章中通往其他頁面、檔案、Email、或其他 URL 的超連結數量 (find_all 找出所有 HTML 標籤 'a' 出現次數)\n",
    "    \n",
    "透過 BeautifulSoup 的 preprocessing 後，總共整理出 15 項資料（見以下表格）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # find title\n",
    "    title = soup.body.h1.string.strip().lower()\n",
    "\n",
    "    # find author\n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    author_name = article_info.find('span', {'class': 'author_name'})\n",
    "    if author_name != None:\n",
    "        author = author_name.get_text()\n",
    "    elif article_info.span != None:\n",
    "        author = article_info.span.string\n",
    "    else:\n",
    "        author = article_info.a.string\n",
    "\n",
    "    # clean author\n",
    "    author = re.sub('\\s+', ' ', author.strip().lower())\n",
    "    if author.startswith('by '):\n",
    "        author = author[3:]\n",
    "    author = re.sub('&.*;', '&', author.replace(' and ', ' & '))\n",
    "\n",
    "    author_list = []\n",
    "    if author.find(',') == -1:\n",
    "        author_list = re.split('\\s*&\\s*', author)\n",
    "    else:\n",
    "        authors = re.split('\\s*,\\s*', author)\n",
    "        if authors[-1].find('&') == -1 or len(authors[-1].split('&')[-1].strip().split()) > 3:\n",
    "            author_list.append(authors[0])\n",
    "        else:\n",
    "            author_list += authors[:-1]\n",
    "            author_list += re.split('\\s*&\\s*', authors[-1])\n",
    "    author = ' '.join([re.sub('\\s+', '_', a) for a in author_list])\n",
    "\n",
    "    # find channel\n",
    "    channel = soup.body.article['data-channel'].strip().lower()\n",
    "\n",
    "    # find topic\n",
    "    a_list = soup.body.find('footer', {'class': 'article-topics'}).find_all('a')\n",
    "    topic_list = [a.string.strip().lower() for a in a_list]\n",
    "    topic = ' '.join([re.sub('\\s+', '_', t) for t in topic_list])\n",
    "\n",
    "    # find datetime\n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    try:\n",
    "        date_time = article_info.time['datetime']\n",
    "    except:\n",
    "        date_time = 'Wed, 10 Oct 2014 15:00:43'\n",
    "    match_obj = re.search('([\\w]+),\\s+([\\d]+)\\s+([\\w]+)\\s+([\\d]+)\\s+([\\d]+):([\\d]+):([\\d]+)', date_time)\n",
    "    day, date, month, year, hour, minute, second = match_obj.groups()\n",
    "    day, month = day.lower(), month.lower()\n",
    "\n",
    "    # find content\n",
    "    content = soup.body.find('section', {'class': 'article-content'}).get_text()\n",
    "    content_len = len(content)\n",
    "\n",
    "    # find see also\n",
    "    num_see_also = len(re.findall('see also', content.lower()))\n",
    "\n",
    "    # find image\n",
    "    num_image = len(soup.body.find_all('img'))\n",
    "\n",
    "    # find a\n",
    "    num_a = len(soup.body.find_all('a'))\n",
    "\n",
    "    return title, author, channel, topic, day, date, month, year, \\\n",
    "        hour, minute, second, content_len, num_see_also, num_image, num_a\n",
    "\n",
    "\n",
    "feature_list = []\n",
    "for text in df_train['Page content']:\n",
    "    feature_list.append(preprocessor(text))\n",
    "for text in df_test['Page content']:\n",
    "    feature_list.append(preprocessor(text))\n",
    "\n",
    "df_combine = pd.DataFrame(\n",
    "    feature_list,\n",
    "    columns=['Title', 'Author', 'Channel', 'Topic', 'Day', 'Date', 'Month', 'Year',\n",
    "             'Hour', 'Minute', 'Second', 'Content_Len', 'Num_See_Also', 'Num_Image', 'Num_A']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Day</th>\n",
       "      <th>Date</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Second</th>\n",
       "      <th>Content_Len</th>\n",
       "      <th>Num_See_Also</th>\n",
       "      <th>Num_Image</th>\n",
       "      <th>Num_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nasa's grand challenge: stop asteroids from de...</td>\n",
       "      <td>clara_moskowitz</td>\n",
       "      <td>world</td>\n",
       "      <td>asteroid asteroids challenge earth space u.s. ...</td>\n",
       "      <td>wed</td>\n",
       "      <td>19</td>\n",
       "      <td>jun</td>\n",
       "      <td>2013</td>\n",
       "      <td>15</td>\n",
       "      <td>04</td>\n",
       "      <td>30</td>\n",
       "      <td>3591</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google's new open source patent pledge: we won...</td>\n",
       "      <td>christina_warren</td>\n",
       "      <td>tech</td>\n",
       "      <td>apps_and_software google open_source opn_pledg...</td>\n",
       "      <td>thu</td>\n",
       "      <td>28</td>\n",
       "      <td>mar</td>\n",
       "      <td>2013</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>55</td>\n",
       "      <td>1843</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ballin': 2014 nfl draft picks get to choose th...</td>\n",
       "      <td>sam_laird</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment nfl nfl_draft sports television</td>\n",
       "      <td>wed</td>\n",
       "      <td>07</td>\n",
       "      <td>may</td>\n",
       "      <td>2014</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>6646</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cameraperson fails deliver slapstick laughs</td>\n",
       "      <td>sam_laird</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>sports video videos watercooler</td>\n",
       "      <td>fri</td>\n",
       "      <td>11</td>\n",
       "      <td>oct</td>\n",
       "      <td>2013</td>\n",
       "      <td>02</td>\n",
       "      <td>26</td>\n",
       "      <td>50</td>\n",
       "      <td>1821</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nfl star helps young fan prove friendship with...</td>\n",
       "      <td>connor_finnegan</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment instagram instagram_video nfl sp...</td>\n",
       "      <td>thu</td>\n",
       "      <td>17</td>\n",
       "      <td>apr</td>\n",
       "      <td>2014</td>\n",
       "      <td>03</td>\n",
       "      <td>31</td>\n",
       "      <td>43</td>\n",
       "      <td>8921</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title            Author  \\\n",
       "0  nasa's grand challenge: stop asteroids from de...   clara_moskowitz   \n",
       "1  google's new open source patent pledge: we won...  christina_warren   \n",
       "2  ballin': 2014 nfl draft picks get to choose th...         sam_laird   \n",
       "3        cameraperson fails deliver slapstick laughs         sam_laird   \n",
       "4  nfl star helps young fan prove friendship with...   connor_finnegan   \n",
       "\n",
       "         Channel                                              Topic  Day Date  \\\n",
       "0          world  asteroid asteroids challenge earth space u.s. ...  wed   19   \n",
       "1           tech  apps_and_software google open_source opn_pledg...  thu   28   \n",
       "2  entertainment      entertainment nfl nfl_draft sports television  wed   07   \n",
       "3    watercooler                    sports video videos watercooler  fri   11   \n",
       "4  entertainment  entertainment instagram instagram_video nfl sp...  thu   17   \n",
       "\n",
       "  Month  Year Hour Minute Second  Content_Len  Num_See_Also  Num_Image  Num_A  \n",
       "0   jun  2013   15     04     30         3591             4          1     21  \n",
       "1   mar  2013   17     40     55         1843             1          1     16  \n",
       "2   may  2014   19     15     20         6646             1          1      9  \n",
       "3   oct  2013   02     26     50         1821             1          0     11  \n",
       "4   apr  2014   03     31     43         8921             1         51     14  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combine.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Feature Extraction\n",
    "接著對每一篇 article 經過 preprocessing 整理的資料進行 **Feature Extraction**。\n",
    "針對時間資訊做以下處理：  \n",
    "Day(Mon, Tue, Wed, Thu, Fri, Sat, Sun) 分別以 1-7 的數字做 mapping；\n",
    "Month(Jan, Feb, Mar, ..., Nov, Dec) 分別以 1-12 的數字做 mapping。\n",
    "\n",
    "最後經過不同 Feature 組合實驗之下，發現時間資訊中的 Minute, Second 以及 Title, Channel, Num_See_Also, Num_Image, Num_a 這些 fearture 對於整體的 score 沒有上升的幫助，因此在實驗過後 drop 掉不必要的 feature，作爲最終 training 的 input  (見以下表格)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_map = {'mon': 1, 'tue': 2, 'wed': 3,\n",
    "           'thu': 4, 'fri': 5, 'sat': 6, 'sun': 7}\n",
    "month_map = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "             'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n",
    "\n",
    "df_copy = df_combine.copy()\n",
    "df_copy['Day'] = df_copy['Day'].map(day_map)\n",
    "df_copy['Month'] = df_copy['Month'].map(month_map)\n",
    "\n",
    "df_copy = df_copy.drop(columns=['Title', 'Channel', 'Minute', 'Second', 'Num_See_Also', 'Num_Image', 'Num_A'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Day</th>\n",
       "      <th>Date</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Content_Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clara_moskowitz</td>\n",
       "      <td>asteroid asteroids challenge earth space u.s. ...</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>2013</td>\n",
       "      <td>15</td>\n",
       "      <td>3591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>christina_warren</td>\n",
       "      <td>apps_and_software google open_source opn_pledg...</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>17</td>\n",
       "      <td>1843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sam_laird</td>\n",
       "      <td>entertainment nfl nfl_draft sports television</td>\n",
       "      <td>3</td>\n",
       "      <td>07</td>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "      <td>19</td>\n",
       "      <td>6646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sam_laird</td>\n",
       "      <td>sports video videos watercooler</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2013</td>\n",
       "      <td>02</td>\n",
       "      <td>1821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>connor_finnegan</td>\n",
       "      <td>entertainment instagram instagram_video nfl sp...</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "      <td>03</td>\n",
       "      <td>8921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author                                              Topic  Day  \\\n",
       "0   clara_moskowitz  asteroid asteroids challenge earth space u.s. ...    3   \n",
       "1  christina_warren  apps_and_software google open_source opn_pledg...    4   \n",
       "2         sam_laird      entertainment nfl nfl_draft sports television    3   \n",
       "3         sam_laird                    sports video videos watercooler    5   \n",
       "4   connor_finnegan  entertainment instagram instagram_video nfl sp...    4   \n",
       "\n",
       "  Date  Month  Year Hour  Content_Len  \n",
       "0   19      6  2013   15         3591  \n",
       "1   28      3  2013   17         1843  \n",
       "2   07      5  2014   19         6646  \n",
       "3   11     10  2013   02         1821  \n",
       "4   17      4  2014   03         8921  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Tokenization and Word Stemming\n",
    "針對文字資訊 (Topic, Author) 做 Tokenization 提取每一個單字，並做 WordNetLemmatizer 來對單字做精度較高的處理 (還原單字原型)，並使用 CountVectorizer 來計算這些資料中單字出現頻率作為 features。  \n",
    "  \n",
    "透過 Column_Transformer 對於指定 feature 做處理，並對每一種 model 傳入不同的 feature 來做 training。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    return re.split('\\s+', text.strip())\n",
    "\n",
    "\n",
    "def tokenizer_wnl(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    text = re.sub(\"([\\w]+)'[\\w]+\",\n",
    "                  (lambda match_obj: match_obj.group(1)), text)\n",
    "    text = re.sub('\\.', '', text)\n",
    "    text = re.sub('[^\\w]+', ' ', text)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(s) for s in re.split('\\s+', text.strip())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "trans_forest = ColumnTransformer(\n",
    "    [('Author', CountVectorizer(tokenizer=tokenizer, lowercase=False), [0]),\n",
    "     ('Topic', CountVectorizer(tokenizer=tokenizer_wnl, lowercase=False), [1])],\n",
    "    n_jobs=-1,\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "trans_other = ColumnTransformer(\n",
    "    [('Author', 'drop', [0]),\n",
    "     ('Topic', CountVectorizer(tokenizer=tokenizer_wnl, lowercase=False), [1])],\n",
    "    n_jobs=-1,\n",
    "    remainder='passthrough'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Selection\n",
    "Feature 準備好了以後進入 model training 階段，並透過實驗選擇較好的參數以及 model 組合，最終用來做 testing set 的 prediction。\n",
    "首先將 training set 分成 80% training set, 20% validation set，調整參數並判斷 training 是否出現 overfitting 的情況。\n",
    "  \n",
    "本次 Competition 嘗試使用的 Model 有 LightGBM Classifier, Random Forest Classifier, XGBoost Classifier, CatBoost Classifier，並透過實驗組合不同 model 觀察其結果的，最終選擇 LightGBM, Random Forest 以及 CatBoost 搭配 Voting Classifier 作為最終的 model，並將 testing data 放入 model 得到 prediction 結果。\n",
    "  \n",
    "以下詳細說明最終使用的 prediction 所使用的 features 以及 models:  \n",
    "- **LightGBM (learning_rate=0.01, n_estimators=300)**  \n",
    "features: Topic, Day, Date, Month, Year, Hour, Content_Len (其中 Topic 使用 CountVectorizer 搭配 tokenizer_wnl 處理)  \n",
    "[train score] 0.67578  \n",
    "[valid score] 0.59656  \n",
    "\n",
    "- **Random Forest (n_estimators=300)**  \n",
    "features: Author, Topic, Day, Date, Month, Year, Hour, Content_Len (其中 Author, Topic 以 CountVectorizer 分別搭配 tokenizer_wnl, tokenizer 處理)  \n",
    "[train score] 1.00000  \n",
    "[valid score] 0.58942  \n",
    "  \n",
    "- **CatBoost (n_estimators=300)**  \n",
    "features: Topic, Day, Date, Month, Year, Hour, Content_Len (其中 Topic 使用 CountVectorizer 搭配 tokenizer_wnl 處理)  \n",
    "[train score] 0.68522  \n",
    "[valid score] 0.58965\n",
    "  \n",
    "- **Voting (voting='soft', weights=[1, 0.2, 0.05])**  \n",
    "以上三種 model 做 voting 得到 training result  \n",
    "[train score] 0.93644  \n",
    "[valid score] 0.59872  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_raw = df_copy.values[:df_train.shape[0]]\n",
    "y_train_raw = (df_train['Popularity'].values == 1).astype(int)\n",
    "X_test = df_copy.values[df_train.shape[0]:]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_raw, y_train_raw, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def training(clf):\n",
    "    cv_results = cross_validate(clf, X_train_raw, y_train_raw,\n",
    "                                scoring='roc_auc', return_train_score=True, return_estimator=True)\n",
    "    print('train score: {:.5f} (+/-{:.5f})'.format(\n",
    "        np.mean(cv_results['train_score']), np.std(cv_results['train_score'])))\n",
    "    print('valid score: {:.5f} (+/-{:.5f})'.format(\n",
    "        np.mean(cv_results['test_score']), np.std(cv_results['test_score'])))\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('train score: {:.5f}'.format(roc_auc_score(\n",
    "        y_train, clf.predict_proba(X_train)[:, 1])))\n",
    "    print('valid score: {:.5f}'.format(roc_auc_score(\n",
    "        y_valid, clf.predict_proba(X_valid)[:, 1])))\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Classifier\n",
    "[Reference](https://lightgbm.readthedocs.io/en/v3.3.3/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.67002 (+/-0.00237)\n",
      "valid score: 0.60292 (+/-0.00819)\n",
      "train score: 0.67156\n",
      "valid score: 0.59803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = Pipeline([('ct', trans_other),\n",
    "                 ('clf', LGBMClassifier(random_state=0, learning_rate=0.009, n_estimators=300))])\n",
    "lgbm = training(lgbm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "[Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.00000 (+/-0.00000)\n",
      "valid score: 0.58562 (+/-0.01088)\n",
      "train score: 1.00000\n",
      "valid score: 0.58640\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = Pipeline([('ct', trans_forest),\n",
    "                   ('clf', RandomForestClassifier(n_jobs=-1, random_state=0, n_estimators=300))])\n",
    "forest = training(forest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier\n",
    "[Reference](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.81768 (+/-0.00317)\n",
      "valid score: 0.58165 (+/-0.01044)\n",
      "train score: 0.81421\n",
      "valid score: 0.56472\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgboost = Pipeline([('ct', trans_other),\n",
    "                    ('clf', XGBClassifier(verbosity=0, n_estimators=300))])\n",
    "xgboost = training(xgboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost Classifier\n",
    "[Reference](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.68785 (+/-0.00297)\n",
      "valid score: 0.59684 (+/-0.00972)\n",
      "train score: 0.68520\n",
      "valid score: 0.59027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost = Pipeline([('ct', trans_other),\n",
    "                     ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=290, learning_rate=0.06))])\n",
    "catboost = training(catboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier\n",
    "[Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.93903 (+/-0.00294)\n",
      "valid score: 0.60309 (+/-0.00960)\n",
      "train score: 0.93632\n",
      "valid score: 0.59879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting = VotingClassifier([('lgbm', lgbm), ('forest', forest), ('catboost', catboost)],\n",
    "                          voting='soft', weights=[1, 0.2, 0.05])\n",
    "voting = training(voting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = voting\n",
    "\n",
    "y_score = best_model.predict_proba(X_test)[:, 1]\n",
    "df_pred = pd.DataFrame({'Id': df_test['Id'], 'Popularity': y_score})\n",
    "df_pred.to_csv('test_pred.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on Kaggle\n",
    "經過多次嘗試不同的 Feature Extraction 與 Preprocessing，Model 的挑選以及參數的 tuning 後，在 Kaggle 平台上獲得了以下成績：  \n",
    "[Public Score]  0.60181 (first place)  \n",
    "[Private Score] 0.59755 (second place)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "這次的 Text Feature Engineering Competition 相當具有挑戰性，除了要在 News article 中以不同的技巧 parse 出可能有幫助的 features，還要針對數字的 data 做 encoding，對文字的 data 做 preprocessing，雖然實驗課中有提到許多對文字 data 的 preprocessing 方式（像是提取 HTML 中去除 tag 的結果、對單字純化的技巧、計算單字出現頻率以及重要性等等方式），但若只是單純將 article 中常見的 文章標題、作者、內容做處理的話很難達到很好的 prediction 結果。  \n",
    "  \n",
    "在多次嘗試各種 features 的可能性及組合，以及天馬行空的 feature 抽取，同時也嘗試對數字 data 做不同方式的 encoding，發現在此次的 popularity prediction 中，時間的資訊相當重要，同時也發現對於某些feature 來說，OneHotEncoder 未必能展現最佳的結果，也是相當有趣的發現。  \n",
    "  \n",
    "最後針對選擇 model 的部分則是使用了 LightGBM 得到了 performance 相當程度的提升，同時也加速了 training 的速度，針對每個 model 所使用的 feature 和參數也不盡相同，最終透過多次實驗才得到了 Public LeaderBoard 第一名的成績。  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9936df39123efbb617b59821fa9903b74f50e310a538397c4da2f638e6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
